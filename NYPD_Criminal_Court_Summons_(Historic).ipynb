{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NYPD Criminal Court Summons (Historic):\n",
        "\n",
        "a. To get the required columns, use this module:\n",
        "\n",
        "\n",
        "1.   get_area_of_interest(df_spark, interested_columns)\n",
        "\n",
        "\n",
        "b. Preprocessing pipeline: Pass your data through these functions. (if your columns fall in those categories)\n",
        "\n",
        "1.   valid_date_check(date)\n",
        "2.   valid_time_check(time)\n",
        "3.   reverse_geo_code_boros(df_spark, Latitude, Longitude, Boro, lat_index, long_index)\n",
        "4.   refine_age_group_race(df_spark, victim_age_group=None, suspect_age_group=None, suspect_race=None, victim_race=None)\n",
        "5.   refine_sex_gender_impute(df_spark, suspect_age=None, suspect_gender=None, victim_age=None, victim_gender=None)\n",
        "6.   refine_precinct_jur(df_spark, precinct=None, Jur_code=None)\n",
        "\n"
      ],
      "metadata": {
        "id": "EtwLWOuvVE1R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8dUqIINUs-T",
        "outputId": "7abcc70a-bc46-44b6-d08c-cba5509e0afa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Collecting openclean\n",
            "  Downloading openclean-0.2.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting openclean-core==0.4.1 (from openclean)\n",
            "  Downloading openclean_core-0.4.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from openclean-core==0.4.1->openclean) (1.0.0)\n",
            "Collecting appdirs>=1.4.4 (from openclean-core==0.4.1->openclean)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from openclean-core==0.4.1->openclean) (2.9.0.post0)\n",
            "Collecting dill (from openclean-core==0.4.1->openclean)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from openclean-core==0.4.1->openclean) (2.32.3)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from openclean-core==0.4.1->openclean) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from openclean-core==0.4.1->openclean) (1.6.1)\n",
            "Requirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from openclean-core==0.4.1->openclean) (4.23.0)\n",
            "Collecting histore>=0.4.0 (from openclean-core==0.4.1->openclean)\n",
            "  Downloading histore-0.4.1-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting flowserv-core>=0.8.0 (from openclean-core==0.4.1->openclean)\n",
            "  Downloading flowserv_core-0.9.4-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jellyfish (from openclean-core==0.4.1->openclean)\n",
            "  Downloading jellyfish-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
            "Collecting refdata>=0.2.0 (from openclean-core==0.4.1->openclean)\n",
            "  Downloading refdata-0.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from openclean-core==0.4.1->openclean) (1.15.2)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (from flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (3.1.44)\n",
            "Collecting paramiko (from flowserv-core>=0.8.0->openclean-core==0.4.1->openclean)\n",
            "  Downloading paramiko-3.5.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting passlib (from flowserv-core>=0.8.0->openclean-core==0.4.1->openclean)\n",
            "  Downloading passlib-1.7.4-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.3.18 in /usr/local/lib/python3.11/dist-packages (from flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (2.0.40)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.11/dist-packages (from flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (8.1.8)\n",
            "Collecting pyyaml-include (from flowserv-core>=0.8.0->openclean-core==0.4.1->openclean)\n",
            "  Downloading pyyaml_include-2.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from histore>=0.4.0->openclean-core==0.4.1->openclean) (5.9.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.2.0->openclean-core==0.4.1->openclean) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.2.0->openclean-core==0.4.1->openclean) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.2.0->openclean-core==0.4.1->openclean) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.2.0->openclean-core==0.4.1->openclean) (0.24.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->openclean-core==0.4.1->openclean) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->openclean-core==0.4.1->openclean) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->openclean-core==0.4.1->openclean) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->openclean-core==0.4.1->openclean) (1.17.0)\n",
            "Collecting datasize>=1.0.0 (from refdata>=0.2.0->openclean-core==0.4.1->openclean)\n",
            "  Downloading datasize-1.0.0.tar.gz (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pooch>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from refdata>=0.2.0->openclean-core==0.4.1->openclean) (1.8.2)\n",
            "Collecting tableprint (from refdata>=0.2.0->openclean-core==0.4.1->openclean)\n",
            "  Downloading tableprint-0.9.1-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->openclean-core==0.4.1->openclean) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->openclean-core==0.4.1->openclean) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->openclean-core==0.4.1->openclean) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->openclean-core==0.4.1->openclean) (2025.4.26)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->openclean-core==0.4.1->openclean) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->openclean-core==0.4.1->openclean) (3.6.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.3.0->refdata>=0.2.0->openclean-core==0.4.1->openclean) (4.3.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.3.0->refdata>=0.2.0->openclean-core==0.4.1->openclean) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=3.2.0->openclean-core==0.4.1->openclean) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.3.18->flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (3.2.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython->flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (4.0.12)\n",
            "Collecting bcrypt>=3.2 (from paramiko->flowserv-core>=0.8.0->openclean-core==0.4.1->openclean)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.11/dist-packages (from paramiko->flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (43.0.3)\n",
            "Collecting pynacl>=1.5 (from paramiko->flowserv-core>=0.8.0->openclean-core==0.4.1->openclean)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: fsspec>=2021.04.0 in /usr/local/lib/python3.11/dist-packages (from pyyaml-include->flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (2025.3.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from tableprint->refdata>=0.2.0->openclean-core==0.4.1->openclean) (0.2.13)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.3->paramiko->flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (1.17.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython->flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (5.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko->flowserv-core>=0.8.0->openclean-core==0.4.1->openclean) (2.22)\n",
            "Downloading openclean-0.2.1-py3-none-any.whl (5.2 kB)\n",
            "Downloading openclean_core-0.4.1-py3-none-any.whl (267 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading flowserv_core-0.9.4-py3-none-any.whl (260 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.8/260.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading histore-0.4.1-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading refdata-0.2.0-py3-none-any.whl (37 kB)\n",
            "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jellyfish-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (356 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.9/356.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading paramiko-3.5.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading passlib-1.7.4-py2.py3-none-any.whl (525 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.6/525.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyyaml_include-2.2-py3-none-any.whl (29 kB)\n",
            "Downloading tableprint-0.9.1-py3-none-any.whl (6.8 kB)\n",
            "Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: datasize\n",
            "  Building wheel for datasize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for datasize: filename=datasize-1.0.0-py2.py3-none-any.whl size=155030 sha256=05ba2b5128e725c803a52c7c147005968f281cf7e24ca826b9c13e535885de3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/a0/e5/4073bad08f2e5258fc86a7caf2df7d3e6ef0118b94f0ad2f1c\n",
            "Successfully built datasize\n",
            "Installing collected packages: passlib, datasize, appdirs, tableprint, pyyaml-include, jellyfish, dill, bcrypt, pynacl, paramiko, refdata, histore, flowserv-core, openclean-core, openclean\n",
            "Successfully installed appdirs-1.4.4 bcrypt-4.3.0 datasize-1.0.0 dill-0.4.0 flowserv-core-0.9.4 histore-0.4.1 jellyfish-1.2.0 openclean-0.2.1 openclean-core-0.4.1 paramiko-3.5.1 passlib-1.7.4 pynacl-1.5.0 pyyaml-include-2.2 refdata-0.2.0 tableprint-0.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install openclean"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing packages required\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import os\n",
        "import requests\n",
        "from six.moves import urllib\n",
        "import sys\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib as plt\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import IPython\n",
        "from IPython import display\n",
        "import sklearn\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from openclean.pipeline import stream\n",
        "from openclean.profiling.column import DefaultColumnProfiler\n",
        "from openclean.data.source.socrata import Socrata\n",
        "from openclean.pipeline import stream\n",
        "from openclean.function.eval.datatype import IsDatetime\n",
        "import datetime\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql.functions import udf, struct\n",
        "from pyspark.sql.types import StringType"
      ],
      "metadata": {
        "id": "fOq89ZjQU0yv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from geopy.geocoders import ArcGIS\n",
        "geocoder=ArcGIS()\n",
        "#example:\n",
        "geocoder.reverse('40.61157006600007, -73.74736517199995')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjam1thSU34J",
        "outputId": "784e4b43-5907-4a63-d6d6-254e56b8db72"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Location(808 Redfern Ave, Far Rockaway, New York 11691, USA, (40.611614718328, -73.747382377557, 0.0))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# API URL\n",
        "fn_src = 'https://data.cityofnewyork.us/resource/sv2w-rv3k.csv?$limit=10000'\n",
        "fn_dst = '/content/NYPD_CSummons_Historic_10k.csv'\n",
        "\n",
        "# Download the file if not already present\n",
        "if not os.path.isfile(fn_dst):\n",
        "    print('Fetching file. This may take a while...', fn_dst)\n",
        "    urllib.request.urlretrieve(fn_src, fn_dst)\n",
        "    print('File has been downloaded:', fn_dst)\n",
        "else:\n",
        "    print('File already exists:', fn_dst)\n",
        "\n",
        "# Initialize Spark\n",
        "sc = SparkContext.getOrCreate()\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "# Read CSV into Spark DataFrame\n",
        "df_spark = spark.read.option(\"header\", True).csv(fn_dst, inferSchema=True)\n",
        "\n",
        "# Summary\n",
        "print(\"Number of rows:\", df_spark.count())\n",
        "df_spark.printSchema()\n",
        "df_spark.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkeHipz9U-Rj",
        "outputId": "a9269980-6064-4213-b7cf-0cf10ab2f541"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching file. This may take a while... /content/NYPD_CSummons_Historic_10k.csv\n",
            "File has been downloaded: /content/NYPD_CSummons_Historic_10k.csv\n",
            "Number of rows: 10000\n",
            "root\n",
            " |-- summons_key: integer (nullable = true)\n",
            " |-- summons_date: timestamp (nullable = true)\n",
            " |-- offense_description: string (nullable = true)\n",
            " |-- law_section_number: string (nullable = true)\n",
            " |-- law_description: string (nullable = true)\n",
            " |-- summons_category_type: string (nullable = true)\n",
            " |-- age_group: string (nullable = true)\n",
            " |-- sex: string (nullable = true)\n",
            " |-- race: string (nullable = true)\n",
            " |-- jurisdiction_code: integer (nullable = true)\n",
            " |-- boro: string (nullable = true)\n",
            " |-- precinct_of_occur: integer (nullable = true)\n",
            " |-- x_coordinate_cd: integer (nullable = true)\n",
            " |-- y_coordinate_cd: integer (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- geocoded_column: string (nullable = true)\n",
            "\n",
            "+-----------+-------------------+--------------------+------------------+-------------------+---------------------+---------+------+--------------------+-----------------+---------+-----------------+---------------+---------------+------------------+------------------+--------------------+\n",
            "|summons_key|       summons_date| offense_description|law_section_number|    law_description|summons_category_type|age_group|   sex|                race|jurisdiction_code|     boro|precinct_of_occur|x_coordinate_cd|y_coordinate_cd|          latitude|         longitude|     geocoded_column|\n",
            "+-----------+-------------------+--------------------+------------------+-------------------+---------------------+---------+------+--------------------+-----------------+---------+-----------------+---------------+---------------+------------------+------------------+--------------------+\n",
            "|  298683858|2024-12-31 00:00:00|FEDERAL MOTOR VEH...|            CFR 49| NYS Transportation|            NYS TRANS|  UNKNOWN|(null)|              (null)|                0| BROOKLYN|               94|        1000101|         202014|  40.7211442432271|-73.94281598864389|POINT (-73.942815...|\n",
            "|  298666497|2024-12-31 00:00:00|ALCOHOLIC BEVERAG...|        10-125(2B)|             (null)|               (null)|    25-44|     M|      WHITE HISPANIC|                0|   QUEENS|              110|        1022285|         212504| 40.74986910551557|-73.86272585671628|POINT (-73.862725...|\n",
            "|  298704193|2024-12-31 00:00:00|REQUIRED EQUIPMEN...|            20-254|             (null)|               (null)|    25-44|     M|               OTHER|                0|MANHATTAN|               13|         987334|         210530|40.744532233902085| -73.9888702675778|POINT (-73.988870...|\n",
            "|  298702254|2024-12-31 00:00:00|      ACCEPT ON HAIL|            19-507|Administrative Code|                  TLC|      65+|     M|               BLACK|                0|   QUEENS|              103|        1037559|         194576| 40.70058232906593|-73.80774181591707|POINT (-73.807741...|\n",
            "|  298778743|2024-12-31 00:00:00|CONSUMPTION OF AL...|              1227|                VTL|                  VTL|    25-44|     M|ASIAN / PACIFIC I...|                0| BROOKLYN|               75|        1020633|         185054| 40.67453243093619|-73.86883651271532|POINT (-73.868836...|\n",
            "+-----------+-------------------+--------------------+------------------+-------------------+---------------------+---------+------+--------------------+-----------------+---------+-----------------+---------------+---------------+------------------+------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#similarly, lets get them into pyspark rdd\n",
        "def get_area_of_interest(df_spark, interested_columns):\n",
        "  df_spark=df_spark.select(interested_columns)\n",
        "  return df_spark"
      ],
      "metadata": {
        "id": "QSORcsBrVfcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Module for date related columns\n",
        "\n",
        "As the dataset is for the data from 2006 to 2025, we can see that there is data from unknown format of \"1010-05-14\" to the year 2025. We need to clean this. Over here, we remove the null values where the complaint date is <2006."
      ],
      "metadata": {
        "id": "up2fhHPMhWXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fileName='1010-05-14 00:00:00'\n",
        "# # matches=re.search(\"([0-9]{4}\\-[0-9]{2}\\-[0-9]{2})\", fileName)\n",
        "# re.search(r'([0-9]{4}\\-[0-9]{2}\\-[0-9]{2})', fileName).group(0)\n",
        "\n",
        "def valid_date_check(date):\n",
        "  if date==None or date==\" \" or date==\"\":\n",
        "      return False\n",
        "  else:\n",
        "    date_cpy=date\n",
        "    date=date.split(\"/\")\n",
        "    try:\n",
        "      month=int(date[0])\n",
        "      day= int(date[1])\n",
        "      year=int(date[2])\n",
        "      if year>=2006 and year<=2020:\n",
        "        try:\n",
        "          refined_date=datetime.datetime(year, month, day)\n",
        "          return True\n",
        "        except:\n",
        "          return False\n",
        "      else:\n",
        "        return False\n",
        "    except:\n",
        "      return False"
      ],
      "metadata": {
        "id": "kml5KmPEeAK4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Module for time related columns\n",
        "\n",
        "Similarly, lets check for the time as well. Here we must have time between\n",
        "the standard 24 hours."
      ],
      "metadata": {
        "id": "3OU8ZkkIhSf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Deleting invalid time\n",
        "def valid_time_check(time):\n",
        "  if time==None or time==\" \" or time==\"\":\n",
        "    return False\n",
        "  else :\n",
        "    cpy_time=time\n",
        "    time=time.split(\":\")\n",
        "    try:\n",
        "      hour=int(time[0])\n",
        "      mins=int(time[1])\n",
        "      secs= int(time[2])\n",
        "      # if hours is 24 then change it to 0 hours\n",
        "      if hour == 24 and mins== 0 and secs == 0:\n",
        "        hour=0\n",
        "      try:\n",
        "        newTime= datetime.time(hour,mins,secs)\n",
        "        return True\n",
        "      except :\n",
        "        return False\n",
        "    except:\n",
        "      return False"
      ],
      "metadata": {
        "id": "6LroG_OQeCe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Module for Age Group and Race columns\n",
        "The module works for only those columns whose column names are passed"
      ],
      "metadata": {
        "id": "3gzYDuVwhO3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def refine_age_group_race(df_spark, victim_age_group=None, suspect_age_group=None, suspect_race=None, victim_race=None):\n",
        "  #params: dataframe, col names for the respective age, gender cols\n",
        "  if victim_age_group:\n",
        "    df_spark = df_spark.na.fill(\"UNKNOWN\",subset=[victim_age_group])\n",
        "  if suspect_age_group:\n",
        "    df_spark = df_spark.na.fill(\"UNKNOWN\",subset=[suspect_age_group])\n",
        "  if suspect_race:\n",
        "    df_spark = df_spark.na.fill(\"UNKNOWN\",subset=[suspect_race])\n",
        "  if victim_race:\n",
        "    df_spark = df_spark.na.fill(\"UNKNOWN\",subset=[victim_race])\n",
        "  return df_spark"
      ],
      "metadata": {
        "id": "7r5AKpmteHSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Module for Gender, Race Columns for suspects and victims\n",
        "\n",
        "The module works for only those columns whose column names are passed"
      ],
      "metadata": {
        "id": "69arM9hIhOIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def refine_sex_gender_impute(df_spark, suspect_age=None, suspect_gender=None, victim_age=None, victim_gender=None):\n",
        "  #params: dataframe, col names for the respective age, gender cols\n",
        "  if suspect_age:\n",
        "    df_spark=df_spark.na.fill(\"U\",subset=[suspect_age])\n",
        "  if victim_age:\n",
        "    df_spark=df_spark.na.fill(\"U\",subset=[victim_age])\n",
        "  if suspect_gender:\n",
        "    df_spark = df_spark.na.fill(\"UNKNOWN\",subset=[suspect_gender])\n",
        "  if victim_gender:\n",
        "    df_spark = df_spark.na.fill(\"UNKNOWN\",subset=[victim_gender])\n",
        "  return df_spark"
      ],
      "metadata": {
        "id": "htQ2JRcseLNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.a: Module for Precinct, Jurisdiction Code:\n",
        "  dropping the null values\n",
        "\n",
        "  The module works for only those columns whose column names are passed along with the df"
      ],
      "metadata": {
        "id": "d_NovxDghKwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_precinct_check(precinct):\n",
        "  if precinct==None or precinct==\" \" or precinct==\"\":\n",
        "    return False\n",
        "  else :\n",
        "    return True\n",
        "\n",
        "def valid_jur_check(jur):\n",
        "  if jur==None or jur==\" \" or jur==\"\":\n",
        "    return False\n",
        "  else :\n",
        "    return True"
      ],
      "metadata": {
        "id": "niQRHl7CeQR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.b Module for Reverse Geocoding the boroughs using latitudes and longitudes.\n",
        "\n",
        "1. First we will remove the rows where latitude, longitude and boroughs are null. (around 450 tuples removed)\n",
        "2. Then, where the boroughs are empty, take the latitude and longitude value and reverse geocode it using the module \"reverseGeocoder\".\n",
        "3. Impute the borough name retrived in the empty space.\n",
        "\n",
        "\n",
        "### USING MASTER DATASET\n",
        "In the case of geocoding, geocoder gives us the zipcodes based on the latitude and longitude values. Inturn, we can use the master dataset of zipcodes inorder to retrive the borough names\n",
        "\n",
        "\n",
        "\n",
        "NOTE: The dataset can be downloaded from : https://data.beta.nyc/en/dataset/pediacities-nyc-neighborhoods/resource/7caac650-d082-4aea-9f9b-3681d568e8a5"
      ],
      "metadata": {
        "id": "YtxVasX_g8OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_geo_code_boros(df_spark, Latitude, Longitude, Boro, lat_index, long_index):\n",
        "  #select data where we have to impute\n",
        "  df_temp_boro_clean=df_spark.filter((df_spark[Latitude].isNotNull()) & (df_spark[Longitude].isNotNull()))\n",
        "  boro_cleaner=df_temp_boro_clean.filter((df_temp_boro_clean[Boro].isNull())|(df_temp_boro_clean[Boro]=='NEW YORK'))\n",
        "\n",
        "  # print(\"We have \"+ str(boro_cleaner.count())+ \" points to impute\")\n",
        "  print(\"___intializing Zip Code Look up ____\")\n",
        "  print(\"____ imputing the points ____\")\n",
        "\n",
        "\n",
        "  #use your path for master dataset here.\n",
        "  df_zips=pd.read_csv(dst)\n",
        "  zip_master={}\n",
        "  zips=df_zips['zip']\n",
        "  boro=df_zips['borough']\n",
        "  for i, j in zip(zips, boro):\n",
        "    zip_master[i]=j\n",
        "  zip_master[10020]='Manhattan'\n",
        "  zip_master[11249]='Brooklyn'\n",
        "\n",
        "  def reverseGeoCoder(latitude, longitude):\n",
        "    loc=geocoder.reverse(str(latitude)+', '+str(longitude), timeout=1000)\n",
        "    zipCode=str(loc).split(\",\")[2][-5:]\n",
        "    if not int(zipCode) in zip_master:\n",
        "      boro=\"UNKNOWN\"\n",
        "    else:\n",
        "      boro=zip_master[int(zipCode)]\n",
        "    boro=boro.upper()\n",
        "    return boro\n",
        "\n",
        "  #creating UD function\n",
        "  ud_func= udf(reverseGeoCoder, StringType())\n",
        "  boro_cleaned_dataframe = boro_cleaner.withColumn(Boro, ud_func(boro_cleaner[lat_index], boro_cleaner[long_index]))\n",
        "\n",
        "  #joining the imputed dataset to the maindataset and returning\n",
        "\n",
        "  joiner_dataset=df_spark.filter((df_spark[Latitude].isNotNull()) & (df_spark[Boro]!='NEW YORK') & (df_spark[Longitude].isNotNull()) & (df_spark[Boro].isNotNull()))\n",
        "  fin_df=joiner_dataset.union(boro_cleaned_dataframe)\n",
        "  return fin_df"
      ],
      "metadata": {
        "id": "tGGvupG0eQsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The size of dataset ~ 5M tuples. So, we need around 5000 data points for 95% confidence level with 1% interval. The size of data is almost 0.1% of the data. So we can get it into our df now"
      ],
      "metadata": {
        "id": "aWqXQ6Xmgz70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark=spark.read.option(\"header\",True).csv(fn_dst,inferSchema=True)\n",
        "df_spark=df_spark.sample(0.0005)\n",
        "df_spark.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq0_3ZPweYnA",
        "outputId": "70dc5cbe-2e5f-4366-88ef-36152e356511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2587"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROFILING TO CHECK FOR NULL VALUES IN ALL THE COLUMNS"
      ],
      "metadata": {
        "id": "vQkebErhmlXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pandasDF = df_spark.toPandas()\n",
        "ds=stream(pandasDF)\n",
        "#Creating profile of our dataset\n",
        "profiles = ds.profile(default_profiler=DefaultColumnProfiler)\n",
        "profiles.stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "Y0ftSoh7mU0y",
        "outputId": "b0bd5443-d0a2-4a97-a3e5-f91322144912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total</th>\n",
              "      <th>empty</th>\n",
              "      <th>distinct</th>\n",
              "      <th>uniqueness</th>\n",
              "      <th>entropy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>SUMMONS_KEY</th>\n",
              "      <td>2587</td>\n",
              "      <td>0</td>\n",
              "      <td>2587</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>11.337064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SUMMONS_DATE</th>\n",
              "      <td>2587</td>\n",
              "      <td>0</td>\n",
              "      <td>1936</td>\n",
              "      <td>0.748357</td>\n",
              "      <td>10.787426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OFFENSE_DESCRIPTION</th>\n",
              "      <td>2587</td>\n",
              "      <td>0</td>\n",
              "      <td>171</td>\n",
              "      <td>0.066100</td>\n",
              "      <td>5.067939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LAW_SECTION_NUMBER</th>\n",
              "      <td>2587</td>\n",
              "      <td>32</td>\n",
              "      <td>149</td>\n",
              "      <td>0.058317</td>\n",
              "      <td>4.924939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LAW_DESCRIPTION</th>\n",
              "      <td>2587</td>\n",
              "      <td>38</td>\n",
              "      <td>18</td>\n",
              "      <td>0.007062</td>\n",
              "      <td>2.244057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SUMMONS_CATEGORY_TYPE</th>\n",
              "      <td>2587</td>\n",
              "      <td>42</td>\n",
              "      <td>48</td>\n",
              "      <td>0.018861</td>\n",
              "      <td>3.837397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AGE_GROUP</th>\n",
              "      <td>2587</td>\n",
              "      <td>43</td>\n",
              "      <td>6</td>\n",
              "      <td>0.002358</td>\n",
              "      <td>1.921182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SEX</th>\n",
              "      <td>2587</td>\n",
              "      <td>46</td>\n",
              "      <td>4</td>\n",
              "      <td>0.001574</td>\n",
              "      <td>0.758864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RACE</th>\n",
              "      <td>2587</td>\n",
              "      <td>211</td>\n",
              "      <td>9</td>\n",
              "      <td>0.003788</td>\n",
              "      <td>0.517321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>JURISDICTION_CODE</th>\n",
              "      <td>2587</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.580238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BORO</th>\n",
              "      <td>2587</td>\n",
              "      <td>294</td>\n",
              "      <td>6</td>\n",
              "      <td>0.002617</td>\n",
              "      <td>2.186225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PRECINCT_OF_OCCUR</th>\n",
              "      <td>2587</td>\n",
              "      <td>0</td>\n",
              "      <td>77</td>\n",
              "      <td>0.029764</td>\n",
              "      <td>6.022957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_COORDINATE_CD</th>\n",
              "      <td>2587</td>\n",
              "      <td>1</td>\n",
              "      <td>1968</td>\n",
              "      <td>0.761021</td>\n",
              "      <td>10.608312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Y_COORDINATE_CD</th>\n",
              "      <td>2587</td>\n",
              "      <td>1</td>\n",
              "      <td>1970</td>\n",
              "      <td>0.761794</td>\n",
              "      <td>10.609567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latitude</th>\n",
              "      <td>2587</td>\n",
              "      <td>1</td>\n",
              "      <td>2000</td>\n",
              "      <td>0.773395</td>\n",
              "      <td>10.638214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Longitude</th>\n",
              "      <td>2587</td>\n",
              "      <td>1</td>\n",
              "      <td>2001</td>\n",
              "      <td>0.773782</td>\n",
              "      <td>10.638988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lon_Lat</th>\n",
              "      <td>2587</td>\n",
              "      <td>1</td>\n",
              "      <td>2001</td>\n",
              "      <td>0.773782</td>\n",
              "      <td>10.638988</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       total  empty  distinct  uniqueness    entropy\n",
              "SUMMONS_KEY             2587      0      2587    1.000000  11.337064\n",
              "SUMMONS_DATE            2587      0      1936    0.748357  10.787426\n",
              "OFFENSE_DESCRIPTION     2587      0       171    0.066100   5.067939\n",
              "LAW_SECTION_NUMBER      2587     32       149    0.058317   4.924939\n",
              "LAW_DESCRIPTION         2587     38        18    0.007062   2.244057\n",
              "SUMMONS_CATEGORY_TYPE   2587     42        48    0.018861   3.837397\n",
              "AGE_GROUP               2587     43         6    0.002358   1.921182\n",
              "SEX                     2587     46         4    0.001574   0.758864\n",
              "RACE                    2587    211         9    0.003788   0.517321\n",
              "JURISDICTION_CODE       2587      0         3    0.001160   0.580238\n",
              "BORO                    2587    294         6    0.002617   2.186225\n",
              "PRECINCT_OF_OCCUR       2587      0        77    0.029764   6.022957\n",
              "X_COORDINATE_CD         2587      1      1968    0.761021  10.608312\n",
              "Y_COORDINATE_CD         2587      1      1970    0.761794  10.609567\n",
              "Latitude                2587      1      2000    0.773395  10.638214\n",
              "Longitude               2587      1      2001    0.773782  10.638988\n",
              "Lon_Lat                 2587      1      2001    0.773782  10.638988"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a. Select the columns that are common with the original dataset:\n",
        "1. SUMMONS_DATE\n",
        "2. AGE_GROUP\n",
        "3. SEX\n",
        "4. RACE\n",
        "5. JURISDICTION_CODE\n",
        "6. BORO\n",
        "7. PRECINCT_OF_OCCUR\n",
        "8. Latitude\n",
        "9. Longitude\n",
        "\n",
        "We can consider the primary key along with this\n",
        "1. SUMMONS_KEY\n"
      ],
      "metadata": {
        "id": "paHwx8COfUAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interested_columns_1=['SUMMONS_KEY', 'SUMMONS_DATE', 'AGE_GROUP', 'SEX', 'RACE', 'JURISDICTION_CODE', 'BORO', 'PRECINCT_OF_OCCUR', 'Latitude', 'Longitude']\n",
        "df_spark=get_area_of_interest(df_spark, interested_columns_1)"
      ],
      "metadata": {
        "id": "GymGzo9SfMN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b. Lets pass the dataset through the preprocessing pipeline"
      ],
      "metadata": {
        "id": "5Ey6YlZ1fdDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp=df_spark.rdd"
      ],
      "metadata": {
        "id": "AFEqyoG1fZSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Date validation"
      ],
      "metadata": {
        "id": "PCwMqIgNfjOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp_=df_temp.map(lambda x:(x, valid_date_check(str(x[1])))).filter(lambda x: x[1]==True)\n",
        "df_temp=df_temp_.map(lambda x: x[0])"
      ],
      "metadata": {
        "id": "eBrU_gyCfgG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Age, Race, Gender Validation"
      ],
      "metadata": {
        "id": "kD2sbi9IAiIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #as this code requires the pyspark dataframe(Not the rdd)\n",
        "df_temp=df_temp.toDF(schema=df_spark.schema)\n",
        "df_temp=refine_age_group_race(df_temp, None, 'AGE_GROUP', 'RACE', None)\n",
        "df_temp=refine_sex_gender_impute(df_temp, None, \"SEX\", None, None)"
      ],
      "metadata": {
        "id": "9AqM1zZbAhKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Geocoding"
      ],
      "metadata": {
        "id": "k0CryG30f201"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#geospacial attributes imputation\n",
        "# df_temp=df_temp.toDF(schema=df_spark.schema)\n",
        "df_spk=reverse_geo_code_boros(df_temp, 'Latitude', 'Longitude', 'BORO', -2, -1)"
      ],
      "metadata": {
        "id": "UCaVenmIfn6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ad93c11-d436-462b-83c9-04687bba0249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "___intializing Zip Code Look up ____\n",
            "____ imputing the points ____\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Jurisdiction Code, Precinct check"
      ],
      "metadata": {
        "id": "Y4scwWysAPhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp=df_spk.rdd\n",
        "df_temp_=df_temp.map(lambda x:(x, valid_precinct_check(x[7]))).filter(lambda x: x[1]==True)\n",
        "df_temp=df_temp_.map(lambda x: x[0])\n",
        "\n",
        "df_temp_=df_temp.map(lambda x:(x, valid_jur_check(x[5]))).filter(lambda x: x[1]==True)\n",
        "df_temp=df_temp_.map(lambda x: x[0])\n",
        "\n",
        "df_spark=df_temp.toDF(schema=df_spark.schema)"
      ],
      "metadata": {
        "id": "Zhx3n4zFAOvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets profile the data now."
      ],
      "metadata": {
        "id": "-K1Yk9XSgcyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pandasDF = df_spk.toPandas()\n",
        "ds=stream(pandasDF)\n",
        "\n",
        "#Creating profile of our dataset\n",
        "profiles = ds.profile(default_profiler=DefaultColumnProfiler)\n",
        "profiles.stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "Xnr8RCLtgZZg",
        "outputId": "3a996037-ed9c-453e-a6c3-d920ae3b407a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total</th>\n",
              "      <th>empty</th>\n",
              "      <th>distinct</th>\n",
              "      <th>uniqueness</th>\n",
              "      <th>entropy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>SUMMONS_KEY</th>\n",
              "      <td>2586</td>\n",
              "      <td>0</td>\n",
              "      <td>2586</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>11.336507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SUMMONS_DATE</th>\n",
              "      <td>2586</td>\n",
              "      <td>0</td>\n",
              "      <td>1935</td>\n",
              "      <td>0.748260</td>\n",
              "      <td>10.786656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AGE_GROUP</th>\n",
              "      <td>2586</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.002320</td>\n",
              "      <td>1.970337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SEX</th>\n",
              "      <td>2586</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0.001933</td>\n",
              "      <td>0.872249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RACE</th>\n",
              "      <td>2586</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0.003480</td>\n",
              "      <td>0.483582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>JURISDICTION_CODE</th>\n",
              "      <td>2586</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.580394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BORO</th>\n",
              "      <td>2586</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.002320</td>\n",
              "      <td>2.132198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PRECINCT_OF_OCCUR</th>\n",
              "      <td>2586</td>\n",
              "      <td>0</td>\n",
              "      <td>77</td>\n",
              "      <td>0.029776</td>\n",
              "      <td>6.022809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latitude</th>\n",
              "      <td>2586</td>\n",
              "      <td>0</td>\n",
              "      <td>2000</td>\n",
              "      <td>0.773395</td>\n",
              "      <td>10.638214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Longitude</th>\n",
              "      <td>2586</td>\n",
              "      <td>0</td>\n",
              "      <td>2001</td>\n",
              "      <td>0.773782</td>\n",
              "      <td>10.638988</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   total  empty  distinct  uniqueness    entropy\n",
              "SUMMONS_KEY         2586      0      2586    1.000000  11.336507\n",
              "SUMMONS_DATE        2586      0      1935    0.748260  10.786656\n",
              "AGE_GROUP           2586      0         6    0.002320   1.970337\n",
              "SEX                 2586      0         5    0.001933   0.872249\n",
              "RACE                2586      0         9    0.003480   0.483582\n",
              "JURISDICTION_CODE   2586      0         3    0.001160   0.580394\n",
              "BORO                2586      0         6    0.002320   2.132198\n",
              "PRECINCT_OF_OCCUR   2586      0        77    0.029776   6.022809\n",
              "Latitude            2586      0      2000    0.773395  10.638214\n",
              "Longitude           2586      0      2001    0.773782  10.638988"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NOTE: as per the \"reverse_geo_code_boros\" module, BORO Values are categorized into 'UNKOWNN' values where Latitudes and Longitudes are NULL. This means, we are NOT removing the rows where BORO is NULL."
      ],
      "metadata": {
        "id": "b-NYhxEHCoym"
      }
    }
  ]
}